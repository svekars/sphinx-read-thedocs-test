<!DOCTYPE html>
<html lang="en">



<head>
  <meta name="robots" content="noindex">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" type="text/css" href="../_static/css/bootstrap.min.css" />
    <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
    <script type="text/javascript" src="../_static/js/bootstrap.min.js"></script>
    <script type="text/javascript" src="../_static/js/star-rating.js"></script>
    <script type="text/javascript" src="../_static/js/left-sidebar2.js"></script>
    <script type="text/javascript" src="../_static/js/jquery.min.js"></script>
    <script type="text/javascript" src="../_static/js/popper.min.js"></script>
    <script type="text/javascript" src="../_static/js/send-feedback.js"></script>
    <script type="text/javascript" src="../_static/js/fetch-version.js"></script>
    <link rel="preload" href="../_static/fonts/FreightSans/freight-sans-book.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-Medium.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/FreightSans/freight-sans-medium-italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="../_static/fonts/IBMPlexMono/IBMPlexMono-SemiBold.woff2" as="font" type="font/woff2" crossorigin="anonymous">

<!-- Preload the katex fonts -->

<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Math-Italic.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Main-Bold.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size1-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size4-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size2-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Size3-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/katex@0.10.0/dist/fonts/KaTeX_Caligraphic-Regular.woff2" as="font" type="font/woff2" crossorigin="anonymous">
</head>






  <div class="header-container">
    <div class="container-xxl header-holder tutorials-header" id="header-holder">
      <div class="header-logo-container">
       <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
        </div>
         <div class="main-menu">
          <ul>
            <li class="main-menu-item">
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
                <a class="with-down-arrow">
                  Learn
                </a>
                <div class="resources-dropdown-menu">
                  <a class="nav-dropdown-item" href="https://pytorch.org/get-started">
                    <span class=dropdown-title>Get Started</span>
                    <p>Run PyTorch locally or get started quickly with one of the supported cloud platforms</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/tutorials">
                    <span class="dropdown-title">Tutorials</span>
                    <p>Whats new in PyTorch tutorials</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/basics/intro.html">
                    <span class="dropdown-title">Learn the Basics</span>
                    <p>Familiarize yourself with PyTorch concepts and modules</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/recipes/recipes_index.html">
                    <span class="dropdown-title">PyTorch Recipes</span>
                    <p>Bite-size, ready-to-deploy PyTorch code examples</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/tutorials/beginner/introyt.html">
                    <span class="dropdown-title">Intro to PyTorch - YouTube Series</span>
                    <p>Master PyTorch basics with our engaging YouTube tutorial series</p>
                  </a>
                </div>
            </li>

            <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
                <a class="with-down-arrow">
                  Ecosystem
                </a>
                <div class="resources-dropdown-menu">
                  <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                    <span class="dropdown-title">Tools</span>
                    <p>Learn about the tools and frameworks in the PyTorch Ecosystem</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/#community-module">
                    <span class=dropdown-title>Community</span>
                    <p>Join the PyTorch developer community to contribute, learn, and get your questions answered</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://discuss.pytorch.org/" target="_blank">
                    <span class=dropdown-title>Forums</span>
                    <p>A place to discuss PyTorch code, issues, install, research</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/resources">
                    <span class=dropdown-title>Developer Resources</span>
                    <p>Find resources and get questions answered</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem/contributor-awards-2023">
                    <span class="dropdown-title">Contributor Awards - 2023</span>
                    <p>Award winners announced at this year's PyTorch Conference</p>
                  </a>
                </div>
              </div>
            </li>

            <li>
            <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
                <a class="with-down-arrow">
                  Edge
                </a>
                <div class="resources-dropdown-menu">
                  <a class="nav-dropdown-item" href="https://pytorch.org/edge">
                    <span class="dropdown-title">About PyTorch Edge</span>
                    <p>Build innovative and privacy-aware AI experiences for edge devices</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/executorch-overview">
                    <span class="dropdown-title">ExecuTorch</span>
                    <p>End-to-end solution for enabling on-device inference capabilities across mobile and edge devices</p>
                  </a>
                </div>
              </div>
            </li>

            <li class="main-menu-item">
              <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
                <a class="with-down-arrow">
                  Docs
                </a>
                <div class="resources-dropdown-menu">
                  <a class="nav-dropdown-item" href="https://pytorch.org/docs/stable/index.html">
                    <span class="dropdown-title">PyTorch</span>
                    <p>Explore the documentation for comprehensive guidance on how to use PyTorch</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/pytorch-domains">
                    <span class="dropdown-title">PyTorch Domains</span>
                    <p>Read the PyTorch Domains documentation to learn more about domain-specific libraries</p>
                  </a>
                </div>
              </div>
            </li>

            <li>
              <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
                <a class="with-down-arrow">
                  Blogs & News
                </a>
                <div class="resources-dropdown-menu">
                  <a class="nav-dropdown-item" href="https://pytorch.org/blog/">
                    <span class="dropdown-title">PyTorch Blog</span>
                    <p>Catch up on the latest technical news and happenings</p>
                  </a>
                   <a class="nav-dropdown-item" href="https://pytorch.org/community-blog">
                    <span class="dropdown-title">Community Blog</span>
                    <p>Stories from the PyTorch ecosystem</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/videos">
                    <span class="dropdown-title">Videos</span>
                    <p>Learn about the latest PyTorch tutorials, new, and more </p>
                  <a class="nav-dropdown-item" href="https://pytorch.org/community-stories">
                    <span class="dropdown-title">Community Stories</span>
                    <p>Learn how our community solves real, everyday machine learning problems with PyTorch</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/events">
                    <span class="dropdown-title">Events</span>
                    <p>Find events, webinars, and podcasts</p>
                  </a>
              </div>
            </li>

            <li>
              <div id="resourcesDropdownButton" data-toggle="resources-dropdown" class="resources-dropdown">
                <a class="with-down-arrow">
                  About
                </a>
                <div class="resources-dropdown-menu">
                  <a class="nav-dropdown-item" href="https://pytorch.org/foundation">
                    <span class="dropdown-title">PyTorch Foundation</span>
                    <p>Learn more about the PyTorch Foundation</p>
                  </a>
                  <a class="nav-dropdown-item" href="https://pytorch.org/governing-board">
                    <span class="dropdown-title">Governing Board</span>
                    <p></p>
                  </a>
                </div>
              </div>
            </li>

            <li class="main-menu-item">
              <div class="no-dropdown">
                <a href="https://pytorch.org/join" data-cta="join">
                  Become a Member
                </a>
              </div>
            </li>
            <li>
             <div class="main-menu-item">
               <a href="https://github.com/pytorch/pytorch" class="github-icon">
               </a>
             </div>
            </li>
            <!--- TODO: This block adds the search icon to the nav bar. We will enable it later.
            <li>
              <div class="main-menu-item">
               <a href="https://github.com/pytorch/pytorch" class="search-icon">
               </a>
              </div>
            </li>
            --->
          </ul>
        </div>

        <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
      </div>
    </div>


   <div class="top-navigation-bar" role="navigation" aria-label="Top Navigation Bar">
    <div class="container-xxl" id="top-navigation-bar">
     <div class="nav-container">
          <div class="nav-left">
            <!-- Left-aligned content here -->
            <ul class="nav-links">
                
                    <li><a href="../index.html">Home</a></li>
                
                    <li><a href="../tutorials.html">Tutorials</a></li>
                
            </ul>
          </div>
          <div class="nav-right">
             <div class="nav-right-left">
                <!-- Intentionally left empty -->
             </div>
             <div class="nav-right-right">
                <!-- Content here will be aligned to the right within the right half -->
                <div class="custom-version-dropdown">
                    <button class="dropdown-toggle">Loading...</button> <!-- Initial text while loading -->
                <ul class="dropdown-menu">
                   <!-- Options will be populated by JavaScript -->
                </ul>
                </div>
                <button id="dark-mode-toggle" class="btn btn-sm btn-outline-secondary">
                  <i id="toggle-icon" class="fas fa-moon" aria-hidden="true"></i>
                </button>
                  <a href="https://github.com/pytorch/pytorch_sphinx_theme" class="github-icon-link" target="_blank">
                   <i class="fab fa-github" aria-hidden="true"></i>
                  </a>
              </div>
          </div>
        </div>
     </div>
</div>




<div class="table-of-contents-link-wrapper">
  <span>Table of Contents</span>
  <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
</div>

<body class="pytorch-body" data-feedback-url="https://github.com/pytorch/pytorch/">
  <div class="container-xxl">
    <div class="container">
     <div class="row">
        <div class="col-md-2 primary-sidebar-border">
          <div class="primary-sidebar-flex-container">
            <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
              <!-- Left column for TOC -->
              <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
   <div class="pytorch-side-scroll">
      <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
         <div class="pytorch-left-menu-search">
          

          

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            <div class="toctree-wrapper">
               <p class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../page1.html">Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/test1.html">Test Page 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../intro/test2.html">Test 2 page</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../page2.html">Getting Started</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../intro/subintro-page.html">Subintro page home</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../intro/subintro/page1.html">Pytorch 2.4: Getting Started on Intel GPU</a></li>
<li class="toctree-l3"><a class="reference internal" href="../intro/subintro/page2.html">FSDP Notes</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../page3.html">Test Markdown Page</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Tutorials</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../tutorials.html">Tutorials</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/tutorial1.html">Tutorial 1</a></li>
<li class="toctree-l2"><a class="reference internal" href="../tutorials/tutorial2.html">Tutorial 2</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Python API</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../torch.html">torch</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../nn.html">torch.nn</a></li>
</ul>

            </div>
          
      </div>
   </div>
</nav>
            </nav>
          </div>
        </div>
        <div class="col-md-8">
          <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
            <!-- Middle column for main content -->
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <div class="row">
               <div class="col-md-8">
                     <ol class="breadcrumb">
    <li class="breadcrumb-item">
        <a href="../index.html">
            Home
        </a>
    </li>
    <li class="breadcrumb-item">
        <a href="../nn.html"
           
               accesskey="U"
               class="active"
           >
           torch.nn
        </a>
    </li>
    <li class="breadcrumb-item active">torch.nn.parallel.DistributedDataParallel</li>
</ol>

               </div>
               <div class="col-md-4">
                     <div class="rating-flex-container">
  <div class="rating-container">
   <div class="rating-prompt">Rate this Page</div>
    <div class="stars-outer">
          <i class="fa-regular fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="fa-regular fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="fa-regular fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="fa-regular fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="fa-regular fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
    </div>
  </div>
 </div>
</div>
               </div>
               <div class="row"></div>
                  <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
                    <section id="torch-nn-parallel-distributeddataparallel">
<span id="torch-nn-parallel-distributeddataparallel-top"></span><h1>torch.nn.parallel.DistributedDataParallel<a class="headerlink" href="#torch-nn-parallel-distributeddataparallel" title="Link to this heading">¶</a></h1>
<dl class="py function">
<dt class="sig sig-object py" id="torch.nn.parallel.DistributedDataParallel">
<span class="sig-prename descclassname"><span class="pre">torch.nn.parallel.</span></span><span class="sig-name descname"><span class="pre">DistributedDataParallel</span></span><span class="sig-paren">(</span><em class="sig-param"><span class="n"><span class="pre">module</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_ids</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">output_device</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">dim</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">0</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">broadcast_buffers</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">True</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">process_group</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">bucket_cap_mb</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">find_unused_parameters</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">check_reduction</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">gradient_as_bucket_view</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">static_graph</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">False</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">delay_all_reduce_named_params</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">param_to_hook_all_reduce</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">mixed_precision</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em>, <em class="sig-param"><span class="n"><span class="pre">device_mesh</span></span><span class="o"><span class="pre">=</span></span><span class="default_value"><span class="pre">None</span></span></em><span class="sig-paren">)</span><a class="reference internal" href="../_modules/torch/nn/parallel/distributed.html#DistributedDataParallel"><span class="viewcode-link"><span class="pre">[source]</span></span></a><a class="headerlink" href="#torch.nn.parallel.DistributedDataParallel" title="Link to this definition">¶</a></dt>
<dd><p>Implement distributed data parallelism based on <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> at module level.</p>
<p>This container provides data parallelism by synchronizing gradients
across each model replica. The devices to synchronize across are
specified by the input <code class="docutils literal notranslate"><span class="pre">process_group</span></code>, which is the entire world
by default. Note that <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> does not chunk or
otherwise shard the input across participating GPUs; the user is
responsible for defining how to do so, for example through the use
of a <code class="xref py py-class docutils literal notranslate"><span class="pre">DistributedSampler</span></code>.</p>
<p>See also: <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#distributed-basics" title="(in PyTorch v2.5)"><span>Basics</span></a> and <a class="reference external" href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-nn-ddp-instead" title="(in PyTorch v2.5)"><span>Use nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel</span></a>.
The same constraints on input as in <a class="reference internal" href="torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> apply.</p>
<p>Creation of this class requires that <code class="docutils literal notranslate"><span class="pre">torch.distributed</span></code> to be already
initialized, by calling <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code></a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> is proven to be significantly faster than
<a class="reference internal" href="torch.nn.DataParallel.html#torch.nn.DataParallel" title="torch.nn.DataParallel"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.nn.DataParallel</span></code></a> for single-node multi-GPU data
parallel training.</p>
<p>To use <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> on a host with N GPUs, you should spawn
up <code class="docutils literal notranslate"><span class="pre">N</span></code> processes, ensuring that each process exclusively works on a single
GPU from 0 to N-1. This can be done by either setting
<code class="docutils literal notranslate"><span class="pre">CUDA_VISIBLE_DEVICES</span></code> for every process or by calling:</p>
<dl>
<dt><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">set_device</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p><a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>where i is from 0 to N-1. In each process, you should refer the following
to construct this module:</p>
<dl>
<dt><a href="#id9"><span class="problematic" id="id10">``</span></a><a href="#id11"><span class="problematic" id="id12">`</span></a>python</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">N</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">device_ids</span><span class="o">=</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">output_device</span><span class="o">=</span><span class="n">i</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p><a href="#id13"><span class="problematic" id="id14">``</span></a><a href="#id15"><span class="problematic" id="id16">`</span></a></p>
<p>In order to spawn up multiple processes per node, you can use either
<code class="docutils literal notranslate"><span class="pre">torch.distributed.launch</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.multiprocessing.spawn</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Please refer to <a class="reference external" href="https://pytorch.org/tutorials/beginner/dist_overview.html">PyTorch Distributed Overview</a>
for a brief introduction to all features related to distributed training.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> can be used in conjunction with
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.ZeroRedundancyOptimizer" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributed.optim.ZeroRedundancyOptimizer</span></code></a> to reduce
per-rank optimizer states memory footprint. Please refer to
<a class="reference external" href="https://pytorch.org/tutorials/recipes/zero_redundancy_optimizer.html">ZeroRedundancyOptimizer recipe</a>
for more details.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend is currently the fastest and highly recommended
backend when using GPUs. This applies to both single-node and
multi-node distributed training.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>This module also supports mixed-precision distributed training.
This means that your model can have different types of parameters such
as mixed types of <code class="docutils literal notranslate"><span class="pre">fp16</span></code> and <code class="docutils literal notranslate"><span class="pre">fp32</span></code>, the gradient reduction on these
mixed types of parameters will just work fine.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you use <code class="docutils literal notranslate"><span class="pre">torch.save</span></code> on one process to checkpoint the module,
and <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> on some other processes to recover it, make sure that
<code class="docutils literal notranslate"><span class="pre">map_location</span></code> is configured properly for every process. Without
<code class="docutils literal notranslate"><span class="pre">map_location</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.load</span></code> would recover the module to devices
where the module was saved from.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When a model is trained on <code class="docutils literal notranslate"><span class="pre">M</span></code> nodes with <code class="docutils literal notranslate"><span class="pre">batch=N</span></code>, the
gradient will be <code class="docutils literal notranslate"><span class="pre">M</span></code> times smaller when compared to the same model
trained on a single node with <code class="docutils literal notranslate"><span class="pre">batch=M*N</span></code> if the loss is summed (NOT
averaged as usual) across instances in a batch (because the gradients
between different nodes are averaged). You should take this into
consideration when you want to obtain a mathematically equivalent
training process compared to the local training counterpart. But in most
cases, you can just treat a DistributedDataParallel wrapped model, a
DataParallel wrapped model and an ordinary model on a single GPU as the
same (E.g. using the same learning rate for equivalent batch size).</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Parameters are never broadcast between processes. The module performs
an all-reduce step on gradients and assumes that they will be modified
by the optimizer in all processes in the same way. Buffers
(e.g. BatchNorm stats) are broadcast from the module in process of rank
0, to all other replicas in the system in every iteration.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are using DistributedDataParallel in conjunction with the
<a class="reference external" href="https://pytorch.org/docs/stable/rpc.html#distributed-rpc-framework" title="(in PyTorch v2.5)"><span>Distributed RPC Framework</span></a>, you should always use
<code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.distributed.autograd.backward()</span></code> to compute gradients and
<a class="reference external" href="https://pytorch.org/docs/stable/distributed.optim.html#torch.distributed.optim.DistributedOptimizer" title="(in PyTorch v2.5)"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.distributed.optim.DistributedOptimizer</span></code></a> for optimizing
parameters.</p>
<p>Example:</p>
</div>
<dl>
<dt><a href="#id17"><span class="problematic" id="id18">``</span></a><a href="#id19"><span class="problematic" id="id20">`</span></a>python</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed.autograd</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">dist_autograd</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.nn.parallel</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedDataParallel</span> <span class="k">as</span> <span class="n">DDP</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch</span><span class="w"> </span><span class="kn">import</span> <span class="n">optim</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.optim</span><span class="w"> </span><span class="kn">import</span> <span class="n">DistributedOptimizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span><span class="w"> </span><span class="nn">torch.distributed.rpc</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">rpc</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.distributed.rpc</span><span class="w"> </span><span class="kn">import</span> <span class="n">RRef</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">t2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">((</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rref</span> <span class="o">=</span> <span class="n">rpc</span><span class="o">.</span><span class="n">remote</span><span class="p">(</span><span class="s2">&quot;worker1&quot;</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">add</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">t1</span><span class="p">,</span> <span class="n">t2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ddp_model</span> <span class="o">=</span> <span class="n">DDP</span><span class="p">(</span><span class="n">my_model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Setup optimizer</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">optimizer_params</span> <span class="o">=</span> <span class="p">[</span><span class="n">rref</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">ddp_model</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optimizer_params</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">RRef</span><span class="p">(</span><span class="n">param</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dist_optim</span> <span class="o">=</span> <span class="n">DistributedOptimizer</span><span class="p">(</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">optimizer_params</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">lr</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span>
<span class="gp">&gt;&gt;&gt; </span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">with</span> <span class="n">dist_autograd</span><span class="o">.</span><span class="n">context</span><span class="p">()</span> <span class="k">as</span> <span class="n">context_id</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">pred</span> <span class="o">=</span> <span class="n">ddp_model</span><span class="p">(</span><span class="n">rref</span><span class="o">.</span><span class="n">to_here</span><span class="p">())</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">loss</span> <span class="o">=</span> <span class="n">loss_func</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist_autograd</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">context_id</span><span class="p">,</span> <span class="p">[</span><span class="n">loss</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">dist_optim</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">context_id</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<p><a href="#id21"><span class="problematic" id="id22">``</span></a><a href="#id23"><span class="problematic" id="id24">`</span></a></p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>DistributedDataParallel currently offers limited support for gradient
checkpointing with <code class="xref py py-meth docutils literal notranslate"><span class="pre">torch.utils.checkpoint()</span></code>.
If the checkpoint is done with use_reentrant=False (recommended), DDP
will work as expected without any limitations.
If, however, the checkpoint is done with use_reentrant=True (the default),
DDP will work as expected when there are no unused parameters in the model
and each layer is checkpointed at most once (make sure you are not passing
<cite>find_unused_parameters=True</cite> to DDP). We currently do not support the
case where a layer is checkpointed multiple times, or when there unused
parameters in the checkpointed model.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To let a non-DDP model load a state dict from a DDP model,
<code class="xref py py-meth docutils literal notranslate"><span class="pre">consume_prefix_in_state_dict_if_present()</span></code>
needs to be applied to strip the prefix “module.” in the DDP state dict before loading.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Constructor, forward method, and differentiation of the output (or a
function of the output of this module) are distributed synchronization
points. Take that into account in case different processes might be
executing different code.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all parameters are registered in the model by the
time it is created. No parameters should be added nor removed later.
Same applies to buffers.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module assumes all parameters are registered in the model of each
distributed processes are in the same order. The module itself will
conduct gradient <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> following the reverse order of the
registered parameters of the model. In other words, it is users’
responsibility to ensure that each distributed process has the exact
same model and thus the exact same parameter registration order.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module allows parameters with non-rowmajor-contiguous strides.
For example, your model may contain some parameters whose
<a class="reference internal" href="../tensor_attributes.html#torch.memory_format" title="torch.memory_format"><code class="xref py py-class docutils literal notranslate"><span class="pre">torch.memory_format</span></code></a> is <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>
and others whose format is <code class="docutils literal notranslate"><span class="pre">torch.channels_last</span></code>.  However,
corresponding parameters in different processes must have the
same strides.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>This module doesn’t work with <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.autograd.grad.html#torch.autograd.grad" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.autograd.grad()</span></code></a> (i.e. it will
only work if gradients are to be accumulated in <code class="docutils literal notranslate"><span class="pre">.grad</span></code> attributes of
parameters).</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>If you plan on using this module with a <code class="docutils literal notranslate"><span class="pre">nccl</span></code> backend or a <code class="docutils literal notranslate"><span class="pre">gloo</span></code>
backend (that uses Infiniband), together with a DataLoader that uses
multiple workers, please change the multiprocessing start method to
<code class="docutils literal notranslate"><span class="pre">forkserver</span></code> (Python 3 only) or <code class="docutils literal notranslate"><span class="pre">spawn</span></code>. Unfortunately
Gloo (that uses Infiniband) and NCCL2 are not fork safe, and you will
likely experience deadlocks if you don’t change this setting.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>You should never try to change your model’s parameters after wrapping
up your model with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>. Because, when
wrapping up your model with <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code>, the constructor
of <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> will register the additional gradient
reduction functions on all the parameters of the model itself at the
time of construction. If you change the model’s parameters afterwards,
gradient reduction functions no longer match the correct set of
parameters.</p>
</div>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p>Using <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> in conjunction with the
<a class="reference external" href="https://pytorch.org/docs/stable/rpc.html#distributed-rpc-framework" title="(in PyTorch v2.5)"><span>Distributed RPC Framework</span></a> is experimental and subject to change.</p>
</div>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>module</strong> (<em>Module</em>) – module to be parallelized</p></li>
<li><p><strong>device_ids</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>) – <p>CUDA devices.
1) For single-device modules, <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> can
contain exactly one device id, which represents the only
CUDA device where the input module corresponding to this process resides.
Alternatively, <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> can also be <code class="docutils literal notranslate"><span class="pre">None</span></code>.
2) For multi-device modules and CPU modules,
<code class="docutils literal notranslate"><span class="pre">device_ids</span></code> must be <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p>
<p>When <code class="docutils literal notranslate"><span class="pre">device_ids</span></code> is <code class="docutils literal notranslate"><span class="pre">None</span></code> for both cases,
both the input data for the forward pass and the actual module
must be placed on the correct device.
(default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p>
</p></li>
<li><p><strong>output_device</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#int" title="(in Python v3.13)"><em>int</em></a><em> or </em><a class="reference internal" href="../tensor_attributes.html#torch.device" title="torch.device"><em>torch.device</em></a>) – Device location of output for
single-device CUDA modules. For multi-device modules and
CPU modules, it must be <code class="docutils literal notranslate"><span class="pre">None</span></code>, and the module itself
dictates the output location. (default: <code class="docutils literal notranslate"><span class="pre">device_ids[0]</span></code>
for single-device modules)</p></li>
<li><p><strong>broadcast_buffers</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Flag that enables syncing (broadcasting)
buffers of the module at beginning of the <code class="docutils literal notranslate"><span class="pre">forward</span></code>
function. (default: <code class="docutils literal notranslate"><span class="pre">True</span></code>)</p></li>
<li><p><strong>process_group</strong> – The process group to be used for distributed data
all-reduction. If <code class="docutils literal notranslate"><span class="pre">None</span></code>, the default process group, which
is created by <a class="reference external" href="https://pytorch.org/docs/stable/distributed.html#torch.distributed.init_process_group" title="(in PyTorch v2.5)"><code class="xref py py-func docutils literal notranslate"><span class="pre">torch.distributed.init_process_group()</span></code></a>,
will be used. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>bucket_cap_mb</strong> – <code class="docutils literal notranslate"><span class="pre">DistributedDataParallel</span></code> will bucket parameters into
multiple buckets so that gradient reduction of each
bucket can potentially overlap with backward computation.
<code class="xref py py-attr docutils literal notranslate"><span class="pre">bucket_cap_mb</span></code> controls the bucket size in
MebiBytes (MiB). If <code class="docutils literal notranslate"><span class="pre">None</span></code>, a default size of 25 MiB
will be used. (default: <code class="docutils literal notranslate"><span class="pre">None</span></code>)</p></li>
<li><p><strong>find_unused_parameters</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – Traverse the autograd graph from all
tensors contained in the return value of the
wrapped module’s <code class="docutils literal notranslate"><span class="pre">forward</span></code> function. Parameters
that don’t receive gradients as part of this
graph are preemptively marked as being ready to
be reduced. In addition, parameters that may have
been used in the wrapped module’s <code class="docutils literal notranslate"><span class="pre">forward</span></code>
function but were not part of loss computation and
thus would also not receive gradients are
preemptively marked as ready to be reduced.
(default: <code class="docutils literal notranslate"><span class="pre">False</span></code>)</p></li>
<li><p><strong>check_reduction</strong> – This argument is deprecated.</p></li>
<li><p><strong>gradient_as_bucket_view</strong> (<a class="reference external" href="https://docs.python.org/3/library/functions.html#bool" title="(in Python v3.13)"><em>bool</em></a>) – When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, gradients will be views
pointing to different offsets of <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> communication
buckets. This can reduce peak memory usage, where the
saved memory size will be equal to the total gradients
size. Moreover, it avoids the overhead of copying between
gradients and <code class="docutils literal notranslate"><span class="pre">allreduce</span></code> communication buckets. When
gradients are views, <code class="docutils literal notranslate"><span class="pre">detach_()</span></code> cannot be called on the
gradients. If hitting such errors, please fix it by
referring to the <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad" title="(in PyTorch v2.5)"><code class="xref py py-meth docutils literal notranslate"><span class="pre">zero_grad()</span></code></a>
function in <code class="docutils literal notranslate"><span class="pre">torch/optim/optimizer.py</span></code> as a solution.
Note that gradients will be views after first iteration, so
the peak memory saving should be checked after first iteration.</p></li>
<li><p><strong>static_graph</strong> – <p>When set to <code class="docutils literal notranslate"><span class="pre">True</span></code>, DDP knows the trained graph is
static. Static graph means 1) The set of used and unused
parameters will not change during the whole training loop; in
this case, it does not matter whether users set
<code class="docutils literal notranslate"><span class="pre">find_unused_parameters</span> <span class="pre">=</span> <span class="pre">True</span></code> or not. 2) How the graph is trained
will not change during the whole training loop (meaning there is
no control flow depending on iterations).
When static_graph is set to be <code class="docutils literal notranslate"><span class="pre">True</span></code>, DDP will support cases that
can not be supported in the past:
1) Reentrant backwards.
2) Activation checkpointing multiple times.
3) Activation checkpointing when model has unused parameters.
4) There are model parameters that are outside of forward function.
5) Potentially improve performance when there are unused parameters,
as DDP will not search graph in each iteration to detect unused
parameters when static_graph is set to be <code class="docutils literal notranslate"><span class="pre">True</span></code>.
To check whether you can set static_graph to be <code class="docutils literal notranslate"><span class="pre">True</span></code>, one way is to
check ddp logging data at the end of your previous model training,
if <code class="docutils literal notranslate"><span class="pre">ddp_logging_data.get(&quot;can_set_static_graph&quot;)</span> <span class="pre">==</span> <span class="pre">True</span></code>, mostly you
can set <code class="docutils literal notranslate"><span class="pre">static_graph</span> <span class="pre">=</span> <span class="pre">True</span></code> as well.</p>
<p>Example:</p>
</p></li>
</ul>
</dd>
</dl>
<dl>
<dt><a href="#id25"><span class="problematic" id="id26">``</span></a><a href="#id27"><span class="problematic" id="id28">`</span></a>python</dt><dd><div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model_DDP</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c1"># Training loop</span>
<span class="gp">&gt;&gt;&gt; </span><span class="o">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">ddp_logging_data</span> <span class="o">=</span> <span class="n">model_DDP</span><span class="o">.</span><span class="n">_get_ddp_logging_data</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">static_graph</span> <span class="o">=</span> <span class="n">ddp_logging_data</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;can_set_static_graph&quot;</span><span class="p">)</span>
</pre></div>
</div>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">Parameters<span class="colon">:</span></dt>
<dd class="field-odd"><ul class="simple">
<li><p><strong>delay_all_reduce_named_params</strong> (<a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#list" title="(in Python v3.13)"><em>list</em></a><em> of </em><a class="reference external" href="https://docs.python.org/3/library/stdtypes.html#tuple" title="(in Python v3.13)"><em>tuple</em></a><em> of </em><em>str and torch.nn.Parameter</em>) – a list
of named parameters whose all reduce will be delayed when the gradient of
the parameter specified in <code class="docutils literal notranslate"><span class="pre">param_to_hook_all_reduce</span></code> is ready. Other
arguments of DDP do not apply to named params specified in this argument
as these named params will be ignored by DDP reducer.</p></li>
<li><p><strong>param_to_hook_all_reduce</strong> (<em>torch.nn.Parameter</em>) – a parameter to hook delayed all reduce
of parameters specified in <code class="docutils literal notranslate"><span class="pre">delay_all_reduce_named_params</span></code>.</p></li>
</ul>
</dd>
</dl>
<p><a href="#id29"><span class="problematic" id="id30">``</span></a><a href="#id31"><span class="problematic" id="id32">`</span></a></p>
<dl class="py attribute">
<dt class="sig sig-object py" id="torch.nn.parallel.module">
<span class="sig-prename descclassname"><span class="pre">torch.nn.parallel.</span></span><span class="sig-name descname"><span class="pre">module</span></span><a class="headerlink" href="#torch.nn.parallel.module" title="Link to this definition">¶</a></dt>
<dd><p>the module to be parallelized.</p>
<dl class="field-list simple">
<dt class="field-odd">Type<span class="colon">:</span></dt>
<dd class="field-odd"><p>Module</p>
</dd>
</dl>
</dd></dl>

<p>Example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>```python
</pre></div>
</div>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># xdoctest: +SKIP(&quot;undefined variables&quot;)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">torch</span><span class="o">.</span><span class="n">distributed</span><span class="o">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="n">backend</span><span class="o">=</span><span class="s1">&#39;nccl&#39;</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">init_method</span><span class="o">=</span><span class="s1">&#39;...&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">net</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">parallel</span><span class="o">.</span><span class="n">DistributedDataParallel</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p><a href="#id33"><span class="problematic" id="id34">``</span></a><a href="#id35"><span class="problematic" id="id36">`</span></a></p>
</dd></dl>

</section>

                  </article>
              <hr> <!-- Horizontal line -->
              <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
    
        <a href="torch.nn.DataParallel.html" class="btn btn-white .with-right-arrow" title="torch.nn.DataParallel" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
    
    
        <a href="torch.nn.utils.clip_grad_norm_.html" class="btn btn-white .with-left-arrow float-right" title="clip_grad_norm" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
    
    </div>

  <div class="feedback">
   <div class="rating-flex-container">
  <div class="rating-container">
   <div class="rating-prompt">Rate this Page</div>
    <div class="stars-outer">
          <i class="fa-regular fa-star" title="1 Star" data-behavior="tutorial-rating" data-count="1"></i>
          <i class="fa-regular fa-star" title="2 Stars" data-behavior="tutorial-rating" data-count="2"></i>
          <i class="fa-regular fa-star" title="3 Stars" data-behavior="tutorial-rating" data-count="3"></i>
          <i class="fa-regular fa-star" title="4 Stars" data-behavior="tutorial-rating" data-count="4"></i>
          <i class="fa-regular fa-star" title="5 Stars" data-behavior="tutorial-rating" data-count="5"></i>
    </div>
  </div>
 </div>
</div>
   <div class="feedback-send">
    <a href="#" class="btn with-right-arrow" onclick="openGitHubIssue()">Send feedback</a>
  </div>

  <div role="contentinfo" class="contentinfo-container">
    <p>
        &copy; Copyright 2024, PyTorch Contributors.
    
        Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
       </p>
     
 </div>
</footer>
             </div>
         </div>
          <div class="col-md-2 right-sidebar-border right-navigation">
            <div class="right-nav-bar">
            <!-- Right column for local TOC -->
                <div class="pytorch-right-menu">
    <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
       <ul>
<li><a class="reference internal" href="#">torch.nn.parallel.DistributedDataParallel</a><ul>
<li><a class="reference internal" href="#torch.nn.parallel.DistributedDataParallel"><code class="docutils literal notranslate"><span class="pre">DistributedDataParallel()</span></code></a><ul>
<li><a class="reference internal" href="#torch.nn.parallel.module"><code class="docutils literal notranslate"><span class="pre">module</span></code></a></li>
</ul>
</li>
</ul>
</li>
</ul>

    </div>
</div>
            </div>
          </div>
      </section>
     </div>
  </div>
</div>
</body>



<!-- Custom sidebar content goes here, or leave empty to remove -->


<!-- left empty to remove -->


<!-- left empty to remove -->


 <!-- Begin Footer -->
 <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">Stay up to date</li>
            <li><a href="https://www.facebook.com/pytorch" target="_blank">Facebook</a></li>
            <li><a href="https://twitter.com/pytorch" target="_blank">Twitter</a></li>
            <li><a href="https://www.youtube.com/pytorch" target="_blank">YouTube</a></li>
            <li><a href="https://www.linkedin.com/company/pytorch" target="_blank">LinkedIn</a></li>
          </ul>
          </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title">PyTorch Podcasts</li>
            <li><a href="https://open.spotify.com/show/6UzHKeiy368jKfQMKKvJY5" target="_blank">Spotify</a></li>
            <li><a href="https://podcasts.apple.com/us/podcast/pytorch-developer-podcast/id1566080008" target="_blank">Apple</a></li>
            <li><a href="https://www.google.com/podcasts?feed=aHR0cHM6Ly9mZWVkcy5zaW1wbGVjYXN0LmNvbS9PQjVGa0lsOA%3D%3D" target="_blank">Google</a></li>
            <li><a href="https://music.amazon.com/podcasts/7a4e6f0e-26c2-49e9-a478-41bd244197d0/PyTorch-Developer-Podcast?" target="_blank">Amazon</a></li>
          </ul>
         </div>
        </div>

        <div class="privacy-policy">
          <ul>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/terms/" target="_blank">Terms</a></li>
            <li class="privacy-policy-links">|</li>
            <li class="privacy-policy-links"><a href="https://www.linuxfoundation.org/privacy-policy/" target="_blank">Privacy</a></li>
          </ul>
        </div>
        <div class="copyright">
        <p>© Copyright The Linux Foundation. The PyTorch Foundation is a project of The Linux Foundation.
          For web site terms of use, trademark policy and other policies applicable to The PyTorch Foundation please see
          <a href="https://www.linuxfoundation.org/policies/">www.linuxfoundation.org/policies/</a>. The PyTorch Foundation supports the PyTorch open source
          project, which has been established as PyTorch Project a Series of LF Projects, LLC. For policies applicable to the PyTorch Project a Series of LF Projects, LLC,
          please see <a href="https://www.lfprojects.org/policies/">www.lfprojects.org/policies/</a>.</p>
      </div>
     </div>

  </footer>

  <div class="cookie-banner-wrapper">
  <div class="container">
    <p class="gdpr-notice">To analyze traffic and optimize your experience, we serve cookies on this site. By clicking or navigating, you agree to allow our usage of cookies. As the current maintainers of this site, Facebook’s Cookies Policy applies. Learn more, including about available controls: <a href="https://www.facebook.com/policies/cookies/">Cookies Policy</a>.</p>
    <img class="close-button" src="../_static/images/pytorch-x.svg">
  </div>
</div>

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
             <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
             <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>


    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
           <li class="resources-mobile-menu-title">
             <a>Learn</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/get-started">Get Started</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials">Tutorials</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/basics/intro.html">Learn the Basics</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/recipes/recipes_index.html">PyTorch Recipes</a>
             </li>
             <li>
               <a href="https://pytorch.org/tutorials/beginner/introyt.html">Introduction to PyTorch - YouTube Series</a>
             </li>
           </ul>
           <li class="resources-mobile-menu-title">
             <a>Ecosystem</a>
           </li>
           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/ecosystem">Tools</a>
             </li>
             <li>
               <a href="https://pytorch.org/#community-module">Community</a>
             </li>
             <li>
               <a href="https://discuss.pytorch.org/">Forums</a>
             </li>
             <li>
               <a href="https://pytorch.org/resources">Developer Resources</a>
             </li>
             <li>
               <a href="https://pytorch.org/ecosystem/contributor-awards-2023">Contributor Awards - 2023</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Edge</a>
           </li>

           <ul class="resources-mobile-menu-items">
             <li>
               <a href="https://pytorch.org/edge">About PyTorch Edge</a>
             </li>

             <li>
               <a href="https://pytorch.org/executorch-overview">ExecuTorch</a>
             </li>
           </ul>

           <li class="resources-mobile-menu-title">
             <a>Docs</a>
           </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/docs/stable/index.html">PyTorch</a>
            </li>

            <li>
              <a href="https://pytorch.org/pytorch-domains">PyTorch Domains</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>Blog & News</a>
          </li>

           <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/blog/">PyTorch Blog</a>
            </li>
            <li>
              <a href="https://pytorch.org/community-blog">Community Blog</a>
            </li>

            <li>
              <a href="https://pytorch.org/videos">Videos</a>
            </li>

            <li>
              <a href="https://pytorch.org/community-stories">Community Stories</a>
            </li>
            <li>
              <a href="https://pytorch.org/events">Events</a>
            </li>
          </ul>

          <li class="resources-mobile-menu-title">
            <a>About</a>
          </li>

          <ul class="resources-mobile-menu-items">
            <li>
              <a href="https://pytorch.org/foundation">PyTorch Foundation</a>
            </li>
            <li>
              <a href="https://pytorch.org/governing-board">Governing Board</a>
            </li>
          </ul>
        </ul>
      </div>
    </div>
  </div>

  <!-- End Mobile Menu -->
</body>
  </html>

<script>
  const chevronDownIconPath = "../_static/images/chevron-down-black.svg"
  const chevronRightIconPath = "../_static/images/chevron-right-orange.svg"
</script>